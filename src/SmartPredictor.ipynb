{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "furnished-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_sent_predictor.predictor.SmartPredictor import SmartPredictor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tweet_sent_predictor.transformer.LowerCaseTransformer import LowerCaseTransformer\n",
    "from tweet_sent_predictor.transformer.MentionFlagger import MentionFlagger\n",
    "from tweet_sent_predictor.transformer.NumberFlagger import NumberFlagger\n",
    "from tweet_sent_predictor.transformer.SplitterPunctuation import SplitterPunctuation, split_punctuation\n",
    "from tweet_sent_predictor.transformer.URLFlagger import URLFlagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tweet_sent_predictor.transformer.StopWordFilter import StopWordFilter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # for reproducibility with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adverse-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_col = \"opinion\"\n",
    "brand_col = \"brand\"\n",
    "text_col = \"body\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-chest",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "raising-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweet_sent_predictor/data/train_proper.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "magnetic-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>brand</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neu</td>\n",
       "      <td>apl</td>\n",
       "      <td>20 min line @apple store @short pump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>irr</td>\n",
       "      <td>msf</td>\n",
       "      <td>Nueva tecnología convierte cualquier superfici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neu</td>\n",
       "      <td>ggl</td>\n",
       "      <td>Some people should not post replies in #Google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>I know a few others having same issue RT @Joel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#Microsoft - We put the \"\"backwards\"\" into bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>fuck this see you hoes @ work @WeakTwip @Munnn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#Microsoft, #Adobe lose $13.5bn to piracy: Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>neu</td>\n",
       "      <td>twt</td>\n",
       "      <td>I tried to explain why you would do \"\"The #Twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>Installed io5 - fine on ipad but wiped wife's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#microsoft #careers site is giving errors for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4173 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     opinion brand                                               body\n",
       "0        neu   apl              20 min line @apple store @short pump.\n",
       "1        irr   msf  Nueva tecnología convierte cualquier superfici...\n",
       "2        neu   ggl  Some people should not post replies in #Google...\n",
       "3        neg   apl  I know a few others having same issue RT @Joel...\n",
       "4        neg   msf  #Microsoft - We put the \"\"backwards\"\" into bac...\n",
       "...      ...   ...                                                ...\n",
       "4168     neg   apl  fuck this see you hoes @ work @WeakTwip @Munnn...\n",
       "4169     neg   msf  #Microsoft, #Adobe lose $13.5bn to piracy: Rep...\n",
       "4170     neu   twt  I tried to explain why you would do \"\"The #Twi...\n",
       "4171     neg   apl  Installed io5 - fine on ipad but wiped wife's ...\n",
       "4172     neg   msf  #microsoft #careers site is giving errors for ...\n",
       "\n",
       "[4173 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prime-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"body\"]\n",
    "y = df[\"opinion\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solved-syracuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   20 min line @apple store @short pump.\n",
       "1       Nueva tecnología convierte cualquier superfici...\n",
       "2       Some people should not post replies in #Google...\n",
       "3       I know a few others having same issue RT @Joel...\n",
       "4       #Microsoft - We put the \"\"backwards\"\" into bac...\n",
       "                              ...                        \n",
       "4168    fuck this see you hoes @ work @WeakTwip @Munnn...\n",
       "4169    #Microsoft, #Adobe lose $13.5bn to piracy: Rep...\n",
       "4170    I tried to explain why you would do \"\"The #Twi...\n",
       "4171    Installed io5 - fine on ipad but wiped wife's ...\n",
       "4172    #microsoft #careers site is giving errors for ...\n",
       "Name: body, Length: 4173, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "combined-waters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neu', 'irr', 'neu', ..., 'neu', 'neg', 'neg'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-nitrogen",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-brisbane",
   "metadata": {},
   "source": [
    "### Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unusual-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9374550682961897  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.75568862 0.74131737 0.76526946 0.76139089 0.76498801]\n",
      "0.7577308691968582 7.929321368339956e-05\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    #(\"count\", CountVectorizer(analyzer=lambda x:x),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Bayesian\", MultinomialNB())\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-atlanta",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "behind-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.8924035466091541  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.72215569 0.70658683 0.73532934 0.72781775 0.73381295]\n",
      "0.7251405103462141 0.0001078678840426528\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    #(\"count\", CountVectorizer(analyzer=lambda x:x),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"SVM\", SVC())\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-membership",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "#### Liblinear + One versus rest scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "natural-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9432063263838965  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.74730539 0.74251497 0.78802395 0.78297362 0.77098321]\n",
      "0.7663602291819239 0.0003396753946960874\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"liblinear\")) # good solver for small dataset\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-clothing",
   "metadata": {},
   "source": [
    "#### newton cg + multinomial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "framed-likelihood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9439252336448598  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.74850299 0.74491018 0.78682635 0.78297362 0.77098321]\n",
      "0.7668392710980917 0.000298815366593932\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"newton-cg\"))\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-lithuania",
   "metadata": {},
   "source": [
    "#### Lbfgs + multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "orange-latin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9439252336448598  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.74850299 0.74491018 0.78682635 0.78297362 0.77098321]\n",
      "0.7668392710980917 0.000298815366593932\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-billion",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "asian-exposure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "floppy-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9439252336448598  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.7257485  0.71616766 0.74131737 0.73860911 0.74220624]\n",
      "0.7328097761311909 0.00010422525387583146\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-poster",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "#### Multi softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "several-clock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softmax',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(objective=\"multi:softmax\")\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "palestinian-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:06:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "0.8636472561706207  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:06:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "[0.74131737 0.73053892 0.76886228 0.76738609 0.76738609]\n",
      "0.7550981490256896 0.0002569007687525858\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-eating",
   "metadata": {},
   "source": [
    "#### Multi softprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "blocked-tokyo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(objective=\"multi:softprob\")\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "still-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "0.8636472561706207  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "[0.74131737 0.73053892 0.76886228 0.76738609 0.76738609]\n",
      "0.7550981490256896 0.0002569007687525858\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
