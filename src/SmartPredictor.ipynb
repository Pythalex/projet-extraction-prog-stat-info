{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dramatic-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_sent_predictor.predictor.SmartPredictor import SmartPredictor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tweet_sent_predictor.transformer.LowerCaseTransformer import LowerCaseTransformer\n",
    "from tweet_sent_predictor.transformer.MentionFlagger import MentionFlagger\n",
    "from tweet_sent_predictor.transformer.NumberFlagger import NumberFlagger\n",
    "from tweet_sent_predictor.transformer.SplitterPunctuation import SplitterPunctuation, split_punctuation\n",
    "from tweet_sent_predictor.transformer.URLFlagger import URLFlagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tweet_sent_predictor.transformer.StopWordFilter import StopWordFilter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "universal-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # for reproducibility with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wound-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_col = \"opinion\"\n",
    "brand_col = \"brand\"\n",
    "text_col = \"body\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-gather",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cosmetic-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweet_sent_predictor/data/train_proper.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pharmaceutical-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>brand</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neu</td>\n",
       "      <td>apl</td>\n",
       "      <td>20 min line @apple store @short pump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>irr</td>\n",
       "      <td>msf</td>\n",
       "      <td>Nueva tecnología convierte cualquier superfici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neu</td>\n",
       "      <td>ggl</td>\n",
       "      <td>Some people should not post replies in #Google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>I know a few others having same issue RT @Joel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#Microsoft - We put the \"\"backwards\"\" into bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>fuck this see you hoes @ work @WeakTwip @Munnn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#Microsoft, #Adobe lose $13.5bn to piracy: Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>neu</td>\n",
       "      <td>twt</td>\n",
       "      <td>I tried to explain why you would do \"\"The #Twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>Installed io5 - fine on ipad but wiped wife's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#microsoft #careers site is giving errors for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4173 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     opinion brand                                               body\n",
       "0        neu   apl              20 min line @apple store @short pump.\n",
       "1        irr   msf  Nueva tecnología convierte cualquier superfici...\n",
       "2        neu   ggl  Some people should not post replies in #Google...\n",
       "3        neg   apl  I know a few others having same issue RT @Joel...\n",
       "4        neg   msf  #Microsoft - We put the \"\"backwards\"\" into bac...\n",
       "...      ...   ...                                                ...\n",
       "4168     neg   apl  fuck this see you hoes @ work @WeakTwip @Munnn...\n",
       "4169     neg   msf  #Microsoft, #Adobe lose $13.5bn to piracy: Rep...\n",
       "4170     neu   twt  I tried to explain why you would do \"\"The #Twi...\n",
       "4171     neg   apl  Installed io5 - fine on ipad but wiped wife's ...\n",
       "4172     neg   msf  #microsoft #careers site is giving errors for ...\n",
       "\n",
       "[4173 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "casual-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"body\"]\n",
    "y = df[\"opinion\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wicked-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   20 min line @apple store @short pump.\n",
       "1       Nueva tecnología convierte cualquier superfici...\n",
       "2       Some people should not post replies in #Google...\n",
       "3       I know a few others having same issue RT @Joel...\n",
       "4       #Microsoft - We put the \"\"backwards\"\" into bac...\n",
       "                              ...                        \n",
       "4168    fuck this see you hoes @ work @WeakTwip @Munnn...\n",
       "4169    #Microsoft, #Adobe lose $13.5bn to piracy: Rep...\n",
       "4170    I tried to explain why you would do \"\"The #Twi...\n",
       "4171    Installed io5 - fine on ipad but wiped wife's ...\n",
       "4172    #microsoft #careers site is giving errors for ...\n",
       "Name: body, Length: 4173, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "academic-mason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neu', 'irr', 'neu', ..., 'neu', 'neg', 'neg'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-egypt",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-forestry",
   "metadata": {},
   "source": [
    "### Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "realistic-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.930026359932902  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.74251497 0.74131737 0.75808383 0.76618705 0.75539568]\n",
      "0.752699780295524 9.029041906981325e-05\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    #(\"count\", CountVectorizer(analyzer=lambda x:x),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Bayesian\", MultinomialNB())\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-rebound",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifty-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.8844955667385574  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.70658683 0.70658683 0.7257485  0.73021583 0.72541966]\n",
      "0.7189115294590674 0.0001041367579882217\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    #(\"count\", CountVectorizer(analyzer=lambda x:x),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"SVM\", SVC())\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-operations",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "#### Liblinear + One versus rest scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ordered-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9336208962377187  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.73413174 0.74371257 0.7748503  0.7853717  0.76019185]\n",
      "0.7596516319878229 0.0003596263927795815\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"liblinear\")) # good solver for small dataset\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-impact",
   "metadata": {},
   "source": [
    "#### newton cg + multinomial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "operational-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9341001677450276  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.73652695 0.74491018 0.7760479  0.7853717  0.76019185]\n",
      "0.7606097158201582 0.0003356246998745189\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"newton-cg\"))\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-engine",
   "metadata": {},
   "source": [
    "#### Lbfgs + multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "centered-chosen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9341001677450276  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.73652695 0.74491018 0.7760479  0.7853717  0.76019185]\n",
      "0.7606097158201582 0.0003356246998745189\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-cathedral",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "practical-manor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "expensive-ordinance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.9338605319913731  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.71616766 0.71137725 0.73413174 0.75539568 0.73021583]\n",
      "0.7294576314995906 0.0002397456467539818\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-arlington",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "#### Multi softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "champion-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softmax',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(objective=\"multi:softmax\")\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tested-murray",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "0.8586149053438773  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "[0.73293413 0.73532934 0.76407186 0.76139089 0.77218225]\n",
      "0.7531816941656257 0.0002551356101652854\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-values",
   "metadata": {},
   "source": [
    "#### Multi softprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "progressive-rally",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(objective=\"multi:softprob\")\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "processed-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "0.8586149053438773  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:11:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "[0.73293413 0.73532934 0.76407186 0.76139089 0.77218225]\n",
      "0.7531816941656257 0.0002551356101652854\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3))),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
