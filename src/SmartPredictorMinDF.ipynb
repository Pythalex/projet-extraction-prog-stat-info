{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unlimited-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_sent_predictor.predictor.SmartPredictor import SmartPredictor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tweet_sent_predictor.transformer.LowerCaseTransformer import LowerCaseTransformer\n",
    "from tweet_sent_predictor.transformer.MentionFlagger import MentionFlagger\n",
    "from tweet_sent_predictor.transformer.NumberFlagger import NumberFlagger\n",
    "from tweet_sent_predictor.transformer.SplitterPunctuation import SplitterPunctuation, split_punctuation\n",
    "from tweet_sent_predictor.transformer.URLFlagger import URLFlagger\n",
    "from tweet_sent_predictor.transformer.HashtagToWords import HashtagToWords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from tweet_sent_predictor.transformer.StopWordFilter import StopWordFilter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "magnetic-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # for reproducibility with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compliant-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_col = \"opinion\"\n",
    "brand_col = \"brand\"\n",
    "text_col = \"body\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-duplicate",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dirty-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweet_sent_predictor/data/train_proper.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handled-substitute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>brand</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neu</td>\n",
       "      <td>apl</td>\n",
       "      <td>20 min line @apple store @short pump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>irr</td>\n",
       "      <td>msf</td>\n",
       "      <td>Nueva tecnología convierte cualquier superfici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neu</td>\n",
       "      <td>ggl</td>\n",
       "      <td>Some people should not post replies in #Google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>I know a few others having same issue RT @Joel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#Microsoft - We put the \"\"backwards\"\" into bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>fuck this see you hoes @ work @WeakTwip @Munnn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#Microsoft, #Adobe lose $13.5bn to piracy: Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>neu</td>\n",
       "      <td>twt</td>\n",
       "      <td>I tried to explain why you would do \"\"The #Twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>neg</td>\n",
       "      <td>apl</td>\n",
       "      <td>Installed io5 - fine on ipad but wiped wife's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>neg</td>\n",
       "      <td>msf</td>\n",
       "      <td>#microsoft #careers site is giving errors for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4173 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     opinion brand                                               body\n",
       "0        neu   apl              20 min line @apple store @short pump.\n",
       "1        irr   msf  Nueva tecnología convierte cualquier superfici...\n",
       "2        neu   ggl  Some people should not post replies in #Google...\n",
       "3        neg   apl  I know a few others having same issue RT @Joel...\n",
       "4        neg   msf  #Microsoft - We put the \"\"backwards\"\" into bac...\n",
       "...      ...   ...                                                ...\n",
       "4168     neg   apl  fuck this see you hoes @ work @WeakTwip @Munnn...\n",
       "4169     neg   msf  #Microsoft, #Adobe lose $13.5bn to piracy: Rep...\n",
       "4170     neu   twt  I tried to explain why you would do \"\"The #Twi...\n",
       "4171     neg   apl  Installed io5 - fine on ipad but wiped wife's ...\n",
       "4172     neg   msf  #microsoft #careers site is giving errors for ...\n",
       "\n",
       "[4173 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reserved-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"body\"]\n",
    "y = df[\"opinion\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consecutive-witch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   20 min line @apple store @short pump.\n",
       "1       Nueva tecnología convierte cualquier superfici...\n",
       "2       Some people should not post replies in #Google...\n",
       "3       I know a few others having same issue RT @Joel...\n",
       "4       #Microsoft - We put the \"\"backwards\"\" into bac...\n",
       "                              ...                        \n",
       "4168    fuck this see you hoes @ work @WeakTwip @Munnn...\n",
       "4169    #Microsoft, #Adobe lose $13.5bn to piracy: Rep...\n",
       "4170    I tried to explain why you would do \"\"The #Twi...\n",
       "4171    Installed io5 - fine on ipad but wiped wife's ...\n",
       "4172    #microsoft #careers site is giving errors for ...\n",
       "Name: body, Length: 4173, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "listed-orbit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neu', 'irr', 'neu', ..., 'neu', 'neg', 'neg'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-electronics",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-census",
   "metadata": {},
   "source": [
    "### Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confirmed-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.7766594775940571  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.73413174 0.71497006 0.74610778 0.73860911 0.74460432]\n",
      "0.7356846020189837 0.00012565208977783233\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    #(\"count\", CountVectorizer(analyzer=lambda x:x),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"Bayesian\", MultinomialNB())\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-double",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "great-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.8658039779535107  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.73293413 0.72215569 0.75568862 0.7470024  0.77697842]\n",
      "0.7469518516922989 0.00035785490788470297\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    #(\"count\", CountVectorizer(analyzer=lambda x:x),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"SVM\", SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-3))\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-niger",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "#### Liblinear + One versus rest scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "massive-deployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.8629283489096573  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.74251497 0.73293413 0.76526946 0.75539568 0.77458034]\n",
      "0.7541389164117807 0.0002256158267353151\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"liblinear\")) # good solver for small dataset\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-river",
   "metadata": {},
   "source": [
    "#### newton cg + multinomial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ancient-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.8804217589264318  accuracy on train set\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "fit\n",
      "[0.73892216 0.72934132 0.76646707 0.74940048 0.77218225]\n",
      "0.7512626545470211 0.0002610208809017463\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"newton-cg\"))\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-helen",
   "metadata": {},
   "source": [
    "#### Lbfgs + multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "expected-alias",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "0.8804217589264318  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "[0.73892216 0.72934132 0.76646707 0.74940048 0.77218225]\n",
      "0.7512626545470211 0.0002610208809017463\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"Maxent\", LogisticRegression(solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-barbados",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "substantial-intelligence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nervous-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "0.8638868919242751  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "[0.73892216 0.72694611 0.7760479  0.75539568 0.77098321]\n",
      "0.753659012909433 0.0003470321324076229\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-ideal",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "#### Multi softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "foreign-thumb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softmax',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(objective=\"multi:softmax\")\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "swiss-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "0.8638868919242751  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "[0.73892216 0.72694611 0.7760479  0.75539568 0.77098321]\n",
      "0.753659012909433 0.0003470321324076229\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-wrestling",
   "metadata": {},
   "source": [
    "#### Multi softprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "universal-algebra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(objective=\"multi:softprob\")\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tight-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "0.8638868919242751  accuracy on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/projet-extraction-prog-stat-info/venv/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fit\n",
      "[0.73892216 0.72694611 0.7760479  0.75539568 0.77098321]\n",
      "0.753659012909433 0.0003470321324076229\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"Lowercase\", LowerCaseTransformer()),\n",
    "    (\"MentionFlagger\", MentionFlagger()),\n",
    "    (\"URLFlagger\", URLFlagger()),\n",
    "    (\"NumberFlagger\", NumberFlagger()),\n",
    "    (\"StopwordFilter\", StopWordFilter()),\n",
    "    (\"HashtagToWords\", HashtagToWords()),\n",
    "    (\"count\", CountVectorizer(analyzer=\"word\", tokenizer=split_punctuation, ngram_range=(1,3), min_df=5)),\n",
    "    (\"RF\", clf) # default\n",
    "])\n",
    "\n",
    "predictor = SmartPredictor(pipe=pipe)\n",
    "predictor.fit(X, y)\n",
    "print(predictor.score(X, y), \" accuracy on train set\")\n",
    "scores = cross_val_score(predictor, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-compact",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
